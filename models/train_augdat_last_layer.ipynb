{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888bab52-af9f-43aa-9eed-e07835a7eba0",
   "metadata": {},
   "source": [
    "# Train Last Layer WITH data augmentation via ImageDataGenerator object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b0695-e1d5-4791-bcd7-be2b97422ce2",
   "metadata": {},
   "source": [
    "The class ImageDataGenerator has objects, like `my_generator` below, can be used as the dataset in `model.fit` just like a tf.dataset can: instead of passing the training set and training labels, we simply pass the ImageDataGenerator object:\n",
    "\n",
    "```python\n",
    "model.fit(my_generator,...)\n",
    "```\n",
    "Here I follow <a href=\"https://stackoverflow.com/questions/56517963/keras-imagedatagenerator-for-segmentation-with-images-and-masks-in-separate-dire\">a stackoverflow</a> post that seems to be slightly dated, since <a href=\"https://stepup.ai/train_data_augmentation_keras/\">this blogpost</a>  says \"**Note** that in previous releases of Keras, the function fit_generator() had to be used instead, but now fit() can handle both types of training!\"\n",
    "\n",
    "never mind... I use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\">tf documentation examplefor segmentation masks</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886e5f9-72c3-4207-9d49-e13856b456f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) sort filenames as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99aecb31-3f30-485c-b6fa-76a159dc5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "## retrieve model\n",
    "# model = keras.models.load_model('saved_models/augdat_last_lay/')\n",
    "\n",
    "## even a pre-trained model needs data:\n",
    "# 1. Sort paths to files\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "path = 'beachlitter/'\n",
    "images = sorted(glob(os.path.join(path, \"images/*.jpg\"))) # list of strings\n",
    "masks = sorted(glob(os.path.join(path, \"maskpngs/*.png\"))) \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sort_paths(path, split=0.1):\n",
    "    # PATH = \"gs://dmcherney/beachlitter/\"\n",
    "    # glob(file_pattern) gives a list of strings satisfying the pattern\n",
    "    images = sorted(glob(os.path.join(path, \"images/*\")))\n",
    "    masks = sorted(glob(os.path.join(path, \"maskpngs/*\")))\n",
    "\n",
    "    total_size = len(images)\n",
    "    valid_size = int(split * total_size)\n",
    "    test_size = int(split * total_size)\n",
    "    \n",
    "    #shuffle with the same random seed to make sure masks stay with images.\n",
    "    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n",
    "\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = sort_paths(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801cde6-e061-474b-9c8a-27797bd3df27",
   "metadata": {},
   "source": [
    "### 1.1) Collect filenames into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72634bab-93fd-4326-b19a-ad51db0f2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38141b1f-6c34-4b67-9987-b1956cccbae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_x</th>\n",
       "      <th>train_y</th>\n",
       "      <th>fake_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beachlitter/images/000711.jpg</td>\n",
       "      <td>beachlitter/maskpngs/000711.png</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beachlitter/images/001911.jpg</td>\n",
       "      <td>beachlitter/maskpngs/001911.png</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beachlitter/images/002911.jpg</td>\n",
       "      <td>beachlitter/maskpngs/002911.png</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beachlitter/images/000652.jpg</td>\n",
       "      <td>beachlitter/maskpngs/000652.png</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beachlitter/images/001688.jpg</td>\n",
       "      <td>beachlitter/maskpngs/001688.png</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         train_x                          train_y  fake_class\n",
       "0  beachlitter/images/000711.jpg  beachlitter/maskpngs/000711.png         1.0\n",
       "1  beachlitter/images/001911.jpg  beachlitter/maskpngs/001911.png         1.0\n",
       "2  beachlitter/images/002911.jpg  beachlitter/maskpngs/002911.png         1.0\n",
       "3  beachlitter/images/000652.jpg  beachlitter/maskpngs/000652.png         1.0\n",
       "4  beachlitter/images/001688.jpg  beachlitter/maskpngs/001688.png         1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_df = pd.DataFrame(columns=['train_x','train_y','fake_class'])\n",
    "path_df['train_x'] = train_x\n",
    "path_df['train_y'] = train_y\n",
    "path_df['fake_class'] = np.ones(path_df['train_x'] .shape) \n",
    "path_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e837b4-0805-4aee-a8e4-e4b28ebe70d7",
   "metadata": {},
   "source": [
    "### 1.2) create two data generators, one for train images, one for train masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aad0afb-0dab-4f73-ac8c-5cdb851d959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2800 validated image filenames.\n",
      "Found 2800 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# make two image data generators with the same seed\n",
    "\n",
    "SEED = 100\n",
    "IMAGE_SIZE = 224\n",
    "BATCH = 8\n",
    "    \n",
    "image_data_generator = ImageDataGenerator( # this has a train/validation split option I'm not using.\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    rotation_range = 10,\n",
    "    zoom_range = 0.1\n",
    "    ).flow_from_dataframe(dataframe= path_df,#Pandas dataframe containing the filepaths relative to directory (or absolute paths if directory is None\n",
    "                          directory=None, # so, absolute\n",
    "                          x_col='train_x', #string, column in dataframe that contains the absolute paths\n",
    "                            y_col='fake_class', #string or list, column/s in dataframe that has the target data. \n",
    "                            weight_col=None,\n",
    "                            target_size=(IMAGE_SIZE, IMAGE_SIZE),#default: (256, 256). The dimensions to which all images found will be resized.\n",
    "                            color_mode='rgb', # one of \"grayscale\", \"rgb\", \"rgba\". Whether the images will be converted to have 1 or 3 color channels.\n",
    "                            classes=None, # optional list of classes \n",
    "                            class_mode=None, #one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None, last meaning generator will only yield batches of image data, which is useful to use in model.predict()\n",
    "                            batch_size=BATCH,\n",
    "                            shuffle=False, #default true, but I already did that, and I want the test set to match the other model fit runs.\n",
    "                            seed=SEED, #for both shuffling and transformations, so I use a seed\n",
    "                            save_to_dir=None, #optionally specify a directory to which to save the augmented pictures\n",
    "                            save_prefix='',\n",
    "                            save_format='png',\n",
    "                            subset=None, # Subset of data (\"training\" or \"validation\") if validation_split is set in ImageDataGenerator.. might do that\n",
    "                            interpolation='nearest', #Interpolation method used to resample the image if the target size is different from that of the loaded image\n",
    "                            validate_filenames=True, #If True, invalid images will be ignored. Disabling this option can lead to speed-up\n",
    "                        )\n",
    "\n",
    "mask_data_generator = ImageDataGenerator(\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    rotation_range = 10,\n",
    "    zoom_range = 0.1\n",
    "    ).flow_from_dataframe(dataframe= path_df,#Pandas dataframe containing the filepaths relative to directory (or absolute paths if directory is None\n",
    "                          directory=None, # so, absolute\n",
    "                          x_col='train_y', #string, column in dataframe that contains the absolute paths\n",
    "                            y_col='fake_class', #string or list, column/s in dataframe that has the target data. \n",
    "                            weight_col=None,\n",
    "                            target_size=(IMAGE_SIZE, IMAGE_SIZE),#default: (256, 256). The dimensions to which all images found will be resized.\n",
    "                            color_mode='rgb', # one of \"grayscale\", \"rgb\", \"rgba\". Whether the images will be converted to have 1 or 3 color channels.\n",
    "                            classes=None, # optional list of classes \n",
    "                            class_mode=None, #one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None, last meaning generator will only yield batches of image data, which is useful to use in model.predict()\n",
    "                            batch_size=BATCH,\n",
    "                            shuffle=False, #default true, but I already did that, and I want the test set to match the other model fit runs.\n",
    "                            seed=SEED, #for both shuffling and transformations, so I use a seed\n",
    "                            save_to_dir=None, #optionally specify a directory to which to save the augmented pictures\n",
    "                            save_prefix='',\n",
    "                            save_format='png',\n",
    "                            subset=None, # Subset of data (\"training\" or \"validation\") if validation_split is set in ImageDataGenerator.. might do that\n",
    "                            interpolation='nearest', #Interpolation method used to resample the image if the target size is different from that of the loaded image\n",
    "                            validate_filenames=True, #If True, invalid images will be ignored. Disabling this option can lead to speed-up\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be9bbc6f-5e99-456f-bdc3-126d5cc08994",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data_generator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143669a-9a62-4dd4-9e3b-4ac597f1c1b1",
   "metadata": {},
   "source": [
    "# how do I see if it is working/where the issue is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00a489-efd6-4483-8518-12fc0f85ade3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60e1ef37-3f9b-46c7-883f-eeb0d02d57ef",
   "metadata": {},
   "source": [
    "### 1.3) Combine generators into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da6cee66-4e0f-40dc-bb07-823915b30468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will put the image and maks data_generators together\n",
    "\n",
    "def my_image_mask_generator(image_data_generator, mask_data_generator):\n",
    "    train_generator = zip(image_data_generator, mask_data_generator)\n",
    "    for (img, mask) in train_generator:\n",
    "        yield (img, mask) # idk how this works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b8499-3f12-44a1-90e7-e4c48186cc5e",
   "metadata": {},
   "source": [
    "**Yield?**        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dc458a4-f244-4e8b-8c77-bf93d080ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_generator = my_image_mask_generator(image_data_generator, mask_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd52cd1-2c4a-4cea-b470-2fdc0bb0416e",
   "metadata": {},
   "source": [
    "## 2) test .fit the model with my_generator as data, with no val data form now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60b2f1da-963e-444d-820f-8808f927f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture and fitting tools\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau \n",
    "from tensorflow.keras.callbacks import TensorBoard \n",
    "# from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import datetime\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf2add-5470-4486-b4c6-1400565baf7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "697b4a4c-f5e8-498c-bb1a-14991af84219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def model():\n",
    "    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n",
    "    \n",
    "    # the pre-trained encoder\n",
    "    encoder = MobileNetV2(input_tensor=inputs, \n",
    "                          weights=\"imagenet\", # instead of randomized initial weights\n",
    "                          include_top=False, # to fully connected softmax layer off. \n",
    "                          alpha=0.35) # proportionally decrease the number of filters in each layer. see paper.\n",
    "    encoder.trainable = False\n",
    "    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \n",
    "                             \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    # output of encoder is 16x16 \n",
    "    \n",
    "    # the decoder follows\n",
    "    f = [16, 32, 48, 64] # the numbers of filters to use in skips traveling UP the U \n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    # output layer:\n",
    "    # old version had one class, thus one filter:\n",
    "    # x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
    "    # x = Activation(\"sigmoid\")(x)\n",
    "    x = Conv2D(8, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"softmax\")(x)\n",
    "\n",
    "    \n",
    "    model = Model(inputs, x) # object created\n",
    "    return model # object returned\n",
    "\n",
    "model = model()\n",
    "LR = 1e-4 #learning rate\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04108efb-b457-4688-be55-fed045e3541f",
   "metadata": {},
   "source": [
    "### 2.2) Test of the .fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3773ac40-bc76-4b69-bd03-8bcd09d3449a",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 473, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 462, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 369, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 664, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 355, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_21249/531230768.py\", line 27, in <module>\n      verbose=1\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 919, in compute_loss\n      y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n      y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/backend.py\", line 5099, in categorical_crossentropy\n      labels=target, logits=output, axis=axis)\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[401408,8] labels_size=[401408,3]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_10069]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21249/3159107787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmy_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# the new guy...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# cites error here, or whatever line is last, so I so not believe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 473, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 462, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 369, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 664, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 355, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_21249/531230768.py\", line 27, in <module>\n      verbose=1\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 919, in compute_loss\n      y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n      y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/backend.py\", line 5099, in categorical_crossentropy\n      labels=target, logits=output, axis=axis)\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[401408,8] labels_size=[401408,3]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_10069]"
     ]
    }
   ],
   "source": [
    "# options first:\n",
    "train_steps = len(train_x)//BATCH\n",
    "\n",
    "#call fit\n",
    "res = model.fit(\n",
    "    my_generator, # the new guy... \n",
    "    epochs = 1,\n",
    "    steps_per_epoch=train_steps, # cites error here, or whatever line is last, so I so not believe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fc89e-bfc9-49df-a297-f6eee22c0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574df5f-a9b3-4a74-bcb0-332e2e8edb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131046e-ecab-40ba-8903-0ef9d6fe2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# scores= pd.DataFrame(data=res.history)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0272e4-00b3-4ce8-9eb3-7766caa949da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.concat([scores,pd.DataFrame(data=res.history)])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fb44c-908a-4236-9237-1f1defc093aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d354f226-9adc-4b3f-85a1-e7682ac4414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/train_with_frozen_encoder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab740265-e4cd-4b17-851b-8b04b06d9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores['accuracy'], label='Train Accuracy')\n",
    "plt.plot(scores['val_accuracy'], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.title(f'Accuracy after {len(scores)} epochs')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297ce26-31b2-46c1-9f63-8a1e230c83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores['loss'], label='Train Loss')\n",
    "plt.plot(scores['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title(f'Accuracy after {len(scores)} epochs')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948cc64-634f-479b-a159-ec9e97823ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e891963-ecba-4e86-b633-b885c91e67c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f744843-1c27-432d-9025-decfe32d5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c35bc-17b7-4b8c-80b4-cb1bbe141c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
